import numpy as np
from scipy.stats import multivariate_normal

class GaussianMixtureModel:
    def __init__(self, n_components, max_iter=100, tol=1e-4):
        self.n_components = n_components
        self.max_iter = max_iter
        self.tol = tol

    def fit(self, X):
        n_samples, n_features = X.shape

        # Initialize parameters
        self.weights = np.ones(self.n_components) / self.n_components
        self.means = X[np.random.choice(n_samples, self.n_components, replace=False)]
        self.covariances = np.tile(np.diag(np.var(X, axis=0)), (self.n_components, 1, 1))

        for _ in range(self.max_iter):
            # Expectation step
            responsibilities = self._compute_responsibilities(X)

            # Maximization step
            prev_weights = self.weights.copy()
            prev_means = self.means.copy()
            prev_covariances = self.covariances.copy()

            self._update_parameters(X, responsibilities)

            # Check convergence
            if np.allclose(self.weights, prev_weights, atol=self.tol) and \
               np.allclose(self.means, prev_means, atol=self.tol) and \
               np.allclose(self.covariances, prev_covariances, atol=self.tol):
                break

    def _compute_responsibilities(self, X):
        n_samples = X.shape[0]
        responsibilities = np.zeros((n_samples, self.n_components))

        for i in range(self.n_components):
            responsibilities[:, i] = self.weights[i] * multivariate_normal.pdf(X, self.means[i], self.covariances[i])

        responsibilities /= np.sum(responsibilities, axis=1, keepdims=True)
        return responsibilities

    def _update_parameters(self, X, responsibilities):
        n_samples = X.shape[0]
        total_resp = np.sum(responsibilities, axis=0)

        # Update weights
        self.weights = total_resp / n_samples

        # Update means
        self.means = np.dot(responsibilities.T, X) / total_resp.reshape(-1, 1)

        # Update covariances
        for i in range(self.n_components):
            diff = X - self.means[i]
            self.covariances[i] = np.dot(responsibilities[:, i] * diff.T, diff) / total_resp[i]

    def predict(self, X):
        responsibilities = self._compute_responsibilities(X)
        return np.argmax(responsibilities, axis=1)

# Example usage
if __name__ == "__main__":
    # Generate synthetic data
    np.random.seed(0)
    n_samples = 300
    means = np.array([[0, 0], [3, 3]])
    covariances = np.array([[[1, 0.5], [0.5, 1]], [[1, -0.5], [-0.5, 1]]])
    X = np.concatenate([np.random.multivariate_normal(mean, cov, int(n_samples / 2)) for mean, cov in zip(means, covariances)])

    # Fit GMM
    gmm = GaussianMixtureModel(n_components=2)
    gmm.fit(X)

    # Predict clusters
    y_pred = gmm.predict(X)

    # Plot clusters
    import matplotlib.pyplot as plt
    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')
    plt.title('Gaussian Mixture Model')
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.show()
